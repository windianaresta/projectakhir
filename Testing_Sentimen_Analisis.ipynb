{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM2attelnSOBn5hpnF+oTCW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/windianaresta/projectakhir/blob/main/Testing_Sentimen_Analisis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install Sastrawi\n",
        "!pip install nltk\n",
        "import pickle\n",
        "from pickle import UnpicklingError\n",
        "import re\n",
        "import string\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
        "import requests\n",
        "import json\n",
        "import nltk\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lc8xPxF8tfkt",
        "outputId": "6843a784-e962-4b13-b127-e124f2056867"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: Sastrawi in /usr/local/lib/python3.11/dist-packages (1.0.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.5.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    with open('/content/svm_model.pkl', 'rb') as f:\n",
        "        svm = pickle.load(f)\n",
        "except (EOFError, UnpicklingError) as e:\n",
        "    print(f\"Error loading xgb model: {e}\")\n",
        "\n",
        "    print(\"Check if 'svm_model.pkl' exists and is not empty.\")\n",
        "    print(\"If the file was transferred, ensure it was done in binary mode.\")\n",
        "\n",
        "\n",
        "try:\n",
        "    with open('/content/tfidf_vectorizer.pkl', 'rb') as f:\n",
        "        tfidf = pickle.load(f)\n",
        "except (EOFError, UnpicklingError) as e:\n",
        "    print(f\"Error loading TF-IDF vectorizer: {e}\")\n",
        "\n",
        "\n",
        "def cleaningText(text):\n",
        "    text = re.sub(r'@[A-Za-z0-9]+', '', text)\n",
        "    text = re.sub(r'#[A-Za-z0-9]+', '', text)\n",
        "    text = re.sub(r\"http\\S+\", '', text)\n",
        "    text = re.sub(r'[0-9]+', '', text)\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "    text = text.replace('\\n', ' ')\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "    text = ' '.join([word for word in text.split() if word.lower() not in [\"shopee\", \"qris\", \"mobile\", \"shopeepay\"]])\n",
        "    text = text.strip(' ')\n",
        "    return text\n",
        "\n",
        "def casefoldingText(text):\n",
        "    text = text.lower()\n",
        "    return text\n",
        "\n",
        "def tokenizingText(text):\n",
        "    text = word_tokenize(text)\n",
        "    return text\n",
        "\n",
        "def filteringText(text):\n",
        "    listStopwords = set(stopwords.words('indonesian'))\n",
        "    listStopwords1 = set(stopwords.words('english'))\n",
        "    listStopwords.update(listStopwords1)\n",
        "    listStopwords.update(['iya','yaa','gak','nya','na','sih','ku','di','ya','loh','kah','deh'])\n",
        "    filtered = []\n",
        "    for txt in text:\n",
        "        if txt not in listStopwords:\n",
        "            filtered.append(txt)\n",
        "    text = filtered\n",
        "    return text\n",
        "\n",
        "def stemmingText(text):\n",
        "    factory = StemmerFactory()\n",
        "    stemmer = factory.create_stemmer()\n",
        "    words = text.split()\n",
        "    stemmed_words = [stemmer.stem(word) for word in words]\n",
        "    stemmed_text = ' '.join(stemmed_words)\n",
        "    return stemmed_text\n",
        "\n",
        "def toSentence(list_words):\n",
        "    sentence = ' '.join(word for word in list_words)\n",
        "    return sentence\n",
        "\n",
        "def fix_slangwords(text):\n",
        "    words = text.split()\n",
        "    fixed_words = []\n",
        "    for word in words:\n",
        "        if word.lower() in slangwords:\n",
        "            fixed_words.append(slangwords[word.lower()])\n",
        "        else:\n",
        "            fixed_words.append(word)\n",
        "    fixed_text = ' '.join(fixed_words)\n",
        "    return fixed_text\n",
        "\n",
        "url = 'https://raw.githubusercontent.com/windianaresta/projectakhir/main/slangwords.json'  # URL tempat kamus slangwords disimpan\n",
        "\n",
        "response = requests.get(url)\n",
        "\n",
        "if response.status_code == 200:\n",
        "    try:\n",
        "        slangwords = json.loads(response.text)\n",
        "    except json.JSONDecodeError as e:\n",
        "        print(\"Error decoding JSON:\", e)\n",
        "        print(\"Response content:\", response.text)\n",
        "else:\n",
        "    print(\"Failed to fetch data from URL. Status code:\", response.status_code)\n",
        "\n",
        "def preprocess_text(text):\n",
        "    text = cleaningText(text)\n",
        "    text = casefoldingText(text)\n",
        "    text = fix_slangwords(text)\n",
        "    text = tokenizingText(text)\n",
        "    text = filteringText(text)\n",
        "    text = toSentence(text)\n",
        "    return text\n",
        "\n",
        "def prediksi_sentimen_kalimat_baru(review_baru, tfidf, svm):\n",
        "\n",
        "    review_baru_cleaned = cleaningText(review_baru)\n",
        "    review_baru_casefolded = casefoldingText(review_baru_cleaned)\n",
        "    review_baru_slangfixed = fix_slangwords(review_baru_casefolded)\n",
        "    review_baru_tokenized = tokenizingText(review_baru_slangfixed)\n",
        "    review_baru_filtered = filteringText(review_baru_tokenized)\n",
        "    review_baru_final = toSentence(review_baru_filtered)\n",
        "\n",
        "\n",
        "    X_review_baru = tfidf.transform([review_baru_final])\n",
        "\n",
        "\n",
        "    X_review_baru = X_review_baru.toarray()\n",
        "\n",
        "\n",
        "    prediksi_sentimen = svm.predict(X_review_baru)\n",
        "\n",
        "\n",
        "    if prediksi_sentimen[0] == 'positive':\n",
        "        hasil = \"Sentimen review baru adalah POSITIF.\"\n",
        "    elif prediksi_sentimen[0] == 'negative':\n",
        "        hasil = \"Sentimen review baru adalah NEGATIF.\"\n",
        "    else:\n",
        "        hasil = \"Sentimen review baru adalah NETRAL.\"\n",
        "\n",
        "    return hasil"
      ],
      "metadata": {
        "id": "MKbo7rSStzMm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "review_baru = \"aneh\"\n",
        "prediksi_sentimen_kalimat_baru(review_baru, tfidf, svm)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "0jr8X8E-t1Pz",
        "outputId": "3c049d95-7730-4496-d850-4677d0605d91"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Sentimen review baru adalah NETRAL.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "review_baru = \"sangat puas\"\n",
        "prediksi_sentimen_kalimat_baru(review_baru, tfidf, svm)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "oJaTJEwYt3Ll",
        "outputId": "bc3dbd12-c593-4bde-f5f4-aa53a09e2ba7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Sentimen review baru adalah POSITIF.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "review_baru = \"jelek\"\n",
        "prediksi_sentimen_kalimat_baru(review_baru, tfidf, svm)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "Jn8yagJAt5bV",
        "outputId": "a494898e-9316-49f0-ef0b-eefe956a8d5b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Sentimen review baru adalah NEGATIF.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    }
  ]
}